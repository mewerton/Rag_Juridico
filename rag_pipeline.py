from typing import List, Dict, Any

import streamlit as st
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document as LCDocument
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents.stuff import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain

from langchain_docling import DoclingLoader
from langchain_docling.loader import ExportType

from pathlib import Path

from utils import (
    log_time,
    prefix_documents_for_e5,
    hash_filename,
    count_tokens,
    extract_metadata,
    format_response,
)

# Configura√ß√µes
DATA_FOLDER = Path("data")
DOCUMENTS_FOLDER = DATA_FOLDER / "documentos"
INDEX_FOLDER = DATA_FOLDER / "indexes"
INDEX_FOLDER.mkdir(parents=True, exist_ok=True)
EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large"
LLM_MODEL_NAME = "llama3-8b-8192"
TOKEN_LIMIT = 7000  # limite aproximado de tokens para prompt


@log_time
def load_documents_with_docling(
    file_path: str,
    export_type: ExportType = ExportType.DOC_CHUNKS
) -> List[LCDocument]:
    """
    1¬∞ - O DoclingLoader l√™ o PDF, extrai o texto estruturado em ‚Äúchunks‚Äù
         e retorna uma lista de Documentos.
    """
    loader = DoclingLoader(file_path=file_path, export_type=export_type)
    return loader.load()


def split_text_into_chunks(documents: List[LCDocument]) -> List[LCDocument]:
    """
    2¬∞ - O RecursiveCharacterTextSplitter divide os documentos em peda√ßos
         menores (300 tokens com 100 de sobreposi√ß√£o) para otimizar embeddings.
    """
    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100)
    return splitter.split_documents(documents)


@log_time
def create_or_load_vectorstore(
    file_path: str,
    documents: List[LCDocument],
    embeddings: HuggingFaceEmbeddings
) -> FAISS:
    """
    3¬∞ - Cria ou carrega um √≠ndice FAISS:
         - Gera um hash a partir do nome do arquivo para nomear a pasta de √≠ndice.
         - Se j√° existir, carrega; caso contr√°rio, cria e salva o novo √≠ndice.
    """
    try:
        # Garante que a pasta existe
        INDEX_FOLDER.mkdir(parents=True, exist_ok=True)

        # Nome base e hash do arquivo para isolar √≠ndices
        base = Path(file_path).stem
        hashed = hash_filename(base)
        index_path = INDEX_FOLDER / f"{hashed}_faiss_index"

        if index_path.exists():
            # Carrega √≠ndice existente
            return FAISS.load_local(
                str(index_path),
                embeddings,
                allow_dangerous_deserialization=True,
            )

        # Cria novo √≠ndice a partir dos documentos
        vectorstore = FAISS.from_documents(documents, embeddings)
        vectorstore.save_local(str(index_path))
        return vectorstore

    except FileNotFoundError as e:
        st.error(f"Arquivo n√£o encontrado: {e}")
        return None
    except ValueError as e:
        st.error(f"Formato inv√°lido: {e}")
        return None


def create_rag_chain(vectorstore: FAISS) -> Any:
    """
    4¬∞ - Cria o retriever com MMR e configura o LLM Groq + Llama3,
         monta a cadeia de documentos e retrieval e retorna um wrapper.
    """
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 20, "fetch_k": 100, "lambda_mult": 0.8},
    )

    llm = ChatGroq(
        temperature=0.1,
        model_name=LLM_MODEL_NAME,
        api_key=st.secrets["GROQ_API_KEY"],
    )
    
    # Prompt padr√£o com contexto e instru√ß√µes
    template = """
Voc√™ √© um assistente jur√≠dico especializado. Analise cuidadosamente o seguinte contexto extra√≠do de documentos jur√≠dicos e responda de forma objetiva, sem adicionar informa√ß√µes externas.

Se n√£o souber a resposta, diga "N√£o encontrei informa√ß√µes suficientes no documento."

<context>
{context}
</context>

Pergunta: {input}

Resposta:
"""
    prompt = ChatPromptTemplate.from_template(template)
    document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)
    retrieval_chain = create_retrieval_chain(retriever, document_chain)

    class RagChainWrapper:
        """
        5¬∞ - Wrapper que:
             a) Estima tokens e alerta se pr√≥ximo do limite
             b) Invoca o chain original
             c) Exibe metadados e trechos do contexto recuperado
             d) Formata a resposta final
        """
        def __init__(self, chain):
            self._chain = chain

        def invoke(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
            # 1) Estima tokens do prompt
            approx = count_tokens(
                template.format(
                    context="",
                    input=inputs.get("input", "")
                ),
                model_name=EMBEDDING_MODEL_NAME
            )
            if approx > TOKEN_LIMIT:
                st.warning(f"‚ö†Ô∏è Prompt estimado em ~{approx} tokens (limite {TOKEN_LIMIT}).")

            # 2) Invoca o chain original
            output = self._chain.invoke(inputs)

            # 3) Exibe metadados e trechos de contexto recuperado
            if "context" in output:
                st.write("üîç Documentos recuperados:")
                for doc in output["context"]:
                    #st.write(f"{extract_metadata(doc)} ‚Äî {doc.page_content[:200]}‚Ä¶")
                    st.write(f"{extract_metadata(doc)} ‚Äî {doc.page_content}")

            # 4) P√≥s‚Äëprocessa a resposta
            if "answer" in output:
                output["answer"] = format_response(output["answer"])

            return output

        __call__ = invoke # permite usar o wrapper como fun√ß√£o

    return RagChainWrapper(retrieval_chain)


@log_time
def process_document(file_path: str) -> Any:
    """
    6¬∞ - Pipeline completo:
         1) Carrega e prefixa documentos
         2) (Opcional) Chunk splitting
         3) Cria/recupera vectorstore FAISS
         4) Constr√≥i e retorna a cadeia RAG personalizada
    """
    # 1) Load & prefix
    docs = load_documents_with_docling(file_path)
    docs = prefix_documents_for_e5(docs)

    # 2) (opcional) split: use este passo somente se
    #    ‚Äì voc√™ estiver carregando o documento como um √∫nico bloco de texto (por ex. via PdfPlumber / PyPDF2),
    #    ‚Äì ou se quiser for√ßar chunks de tamanho fixo diferentes dos produzidos pelo Docling;
    #    Caso contr√°rio, o DoclingLoader com export_type=DOC_CHUNKS j√° retorna trechos bem estruturados
    #    (t√≠tulos, par√°grafos, tabelas) e n√£o vale a pena requebr√°-los com RecursiveCharacterTextSplitter.
    #
    # Para ativar, basta descomentar a linha abaixo:
    # docs = split_text_into_chunks(docs)

    # 3) embeddings e vectorstore
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    vs = create_or_load_vectorstore(file_path, docs, embeddings)

    # 4) Cria e retorna o wrapper da chain
    return create_rag_chain(vs)
